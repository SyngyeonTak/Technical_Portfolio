import random
from collections import Counter
import numpy as np

def make_learnable_dataset_sid(dataset):
    """
    dataset: list of (sequence, session_id, is_original) tuples
    returns: (inputs, targets, session_ids, is_original_flags)
    """
    input_sequences   = [seq[:-1] for seq, sid, is_original in dataset]
    target_items      = [seq[-1] for seq, sid, is_original in dataset]
    session_ids       = [sid for seq, sid, is_original in dataset]
    is_original_flags = [is_original for seq, sid, is_original in dataset]

    return (input_sequences, target_items, session_ids, is_original_flags)

def get_all_item_occurance(dataset):
    all_items = [item for sequence in dataset for item in sequence]

    # Count the occurrences of each item
    occurrences = Counter(all_items)

    # Display the occurrences
    return occurrences

def prefix_cropping_sid(original_dataset):
    """
    original_dataset: list of (sequence, session_id, is_original=True)
    returns: list of (cropped_sequence, session_id, is_original=False)
    """
    prefix_cropping_dataset = []

    for sequence, sid, _ in original_dataset:
        loop_range = len(sequence) - 2
        for idx in range(loop_range):
            modified_sequence = sequence[:-(idx + 1)]
            prefix_cropping_dataset.append((modified_sequence, sid, False))  # prefix â†’ False

    return prefix_cropping_dataset


def meta_hybrid_scores(
    dataset,
    sim_mat,
    attn_log,
    alpha=0.5,
    window_size=5
):
    attn_dict = {idx: weights for idx, weights in attn_log}
    results = []

    half_w = window_size // 2

    for session, session_idx, _ in dataset:
        weights = attn_dict[session_idx]
        session = np.array(session)
        item_scores = []

        for i, (cur_id, w) in enumerate(zip(session[:-1], weights)):
            cur_index = cur_id - 1

            left = max(0, i - half_w)
            right = min(len(session), i + half_w + 1)
            neigh_idx = [session[j] - 1 for j in range(left, right) if j != i]
            context_score = sim_mat[cur_index, neigh_idx].mean()

            rep_score = w
            hybrid_score = alpha * context_score + (1 - alpha) * rep_score

            item_scores.append((cur_id, hybrid_score, i))

        results.append((session_idx, item_scores))

    return results

def binary_search_fdataset_hybrid_wo_rm(
    original_dataset, 
    target_ratio,        # ðŸŽ¯ ëª©í‘œ ë¹„ìœ¨
    sim_mat,
    attn_log,
    alpha=0.5,           # context vs rep_weight ë¹„ìœ¨
    k=1
):
    """
    Hybrid ë°©ì‹ (context + rep_weight) ì´ì§„íƒìƒ‰ í•„í„°ë§.
    """
    augmented_dataset = original_dataset + prefix_cropping_sid(original_dataset)
    full_size = len(augmented_dataset)
    target_count = full_size - (full_size *target_ratio)

    score_set = meta_hybrid_scores(
        original_dataset,
        sim_mat=sim_mat,
        attn_log=attn_log,
        alpha=alpha,
    )

    score_set = make_extracted_sorted(score_set, k=k)

    fdataset_removed, removed_items = remove_items_from_dataset(original_dataset,score_set=score_set,target_count=target_count)

    faugmented_dataset = original_dataset + prefix_cropping_sid(fdataset_removed)

    return faugmented_dataset, removed_items

def make_extracted_sorted(fdataset, k=1):
    """
    fdataset: [(session_idx, [(item_id, hybrid_score, pos), ...]), ...]
    k: ê° ì„¸ì…˜ì—ì„œ ìµœì†Œ hybrid_score ê¸°ì¤€ìœ¼ë¡œ ë½‘ì„ ì•„ì´í…œ ê°œìˆ˜
    descending: Trueë©´ hybrid_score ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬, Falseë©´ ì˜¤ë¦„ì°¨ìˆœ
    """
    extracted = []

    for session_idx, session_result in fdataset:
        sorted_items = sorted(session_result, key=lambda x: x[1])
        for _, score, pos in sorted_items[:k]:
            extracted.append((session_idx, score, pos))

    # hybrid_score ê¸°ì¤€ ì •ë ¬
    extracted_sorted = sorted(
        extracted, key=lambda x: x[1], reverse=False
    )

    return extracted_sorted

from collections import defaultdict
def remove_items_from_dataset(original_dataset, score_set, target_count):
    count = 0

    score_dict = defaultdict(list)
    for session_idx, score, pos in score_set:
        score_dict[session_idx].append([score, pos])
    # pos ê¸°ì¤€ ì •ë ¬ (í˜¹ì€ hybrid_score ê¸°ì¤€ ì •ë ¬)
    for sid in score_dict:
        score_dict[sid].sort(key=lambda x: x[1])  # pos ìˆœì„œë¡œ ì •ë ¬


    # ì„¸ì…˜ ì¸ë±ìŠ¤ë³„ë¡œ ë¹ ë¥¸ ì ‘ê·¼ìš© dict
    session_map = {sid: list(session) for session, sid, _ in original_dataset}
    removed_items = []
    count = 0
    for session_idx, score, pos in score_set:
        if count >= target_count:
            break

        session = session_map[session_idx]
        if len(session) <= 2:
            continue

        # ì œê±°
        removed_item = session[pos]
        new_session = session[:pos] + session[pos+1:]
        if len(new_session) >= 2:
            session_map[session_idx] = new_session
            removed_items.append((session_idx, pos, removed_item, score))
            count += 1

            # ðŸ”‘ pos ì—…ë°ì´íŠ¸: í•´ë‹¹ ì„¸ì…˜ dictì—ì„œë§Œ ì²˜ë¦¬
            for entry in score_dict[session_idx]:
                if entry[1] > pos:
                    entry[1] -= 1  # pos ì•žìœ¼ë¡œ ë•¡ê¸°ê¸°

    # ìµœì¢… dataset ìž¬êµ¬ì„±
    fdataset_removed = [
        (session_map[sid], sid, True)
        for _, sid, _ in original_dataset
        if sid in session_map
    ]

    return fdataset_removed, removed_items

def random_remove_items_from_dataset(original_dataset, target_count):
    # ì„¸ì…˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    session_indices = list(range(len(original_dataset)))
    random.shuffle(session_indices)  # ì„¸ì…˜ ìˆœì„œ ëžœë¤í™”

    # ì„¸ì…˜ ë³µì‚¬ë³¸ (ì›ë³¸ ë³´ì¡´)
    session_map = {i: list(original_dataset[i]) for i in session_indices}

    removed_items = []
    count = 0

    # ê° ì„¸ì…˜ í•œ ë²ˆì”©ë§Œ ì ‘ê·¼
    for sid in session_indices:
        if count >= target_count:
            break

        session = session_map[sid]
        if len(session) <= 2:
            continue  # ë„ˆë¬´ ì§§ì€ ì„¸ì…˜ì€ íŒ¨ìŠ¤

        # ë¬´ìž‘ìœ„ ì•„ì´í…œ ì œê±°
        pos = random.randint(0, len(session) - 1)
        removed_item = session[pos]

        # ì œê±° í›„ ê¸¸ì´ í™•ì¸
        new_session = session[:pos] + session[pos+1:]
        if len(new_session) >= 2:
            session_map[sid] = new_session
            removed_items.append((sid, pos, removed_item))
            count += 1

    # ìµœì¢… dataset ìž¬êµ¬ì„±
    fdataset_removed = [session_map[i] for i in sorted(session_map.keys())]

    return fdataset_removed, removed_items
