import pickle
import random
from collections import Counter
import numpy as np
import os, gzip, pickle

def make_learnable_dataset_sid(dataset):
    """
    dataset: list of (sequence, session_id, is_original) tuples
    returns: (inputs, targets, session_ids, is_original_flags)
    """
    input_sequences   = [seq[:-1] for seq, sid, is_original in dataset]
    target_items      = [seq[-1] for seq, sid, is_original in dataset]
    session_ids       = [sid for seq, sid, is_original in dataset]
    is_original_flags = [is_original for seq, sid, is_original in dataset]

    return (input_sequences, target_items, session_ids, is_original_flags)

def get_all_item_occurance(dataset):
    all_items = [item for sequence in dataset for item in sequence]

    # Count the occurrences of each item
    occurrences = Counter(all_items)

    # Display the occurrences
    return occurrences

def prefix_cropping_sid(original_dataset):
    """
    original_dataset: list of (sequence, session_id, is_original=True)
    returns: list of (cropped_sequence, session_id, is_original=False)
    """
    prefix_cropping_dataset = []

    for sequence, sid, _ in original_dataset:
        loop_range = len(sequence) - 2
        for idx in range(loop_range):
            modified_sequence = sequence[:-(idx + 1)]
            prefix_cropping_dataset.append((modified_sequence, sid, False))  # prefix ‚Üí False

    return prefix_cropping_dataset


def meta_hybrid_scores(
    dataset,
    sim_mat,
    attn_log,
    alpha=0.5,
    window_size=5
):
    attn_dict = {idx: weights for idx, weights in attn_log}
    results = []

    half_w = window_size // 2

    for session, session_idx, _ in dataset:
        weights = attn_dict[session_idx]
        session = np.array(session)
        item_scores = []

        for i, (cur_id, w) in enumerate(zip(session[:-1], weights)):
            cur_index = cur_id - 1

            left = max(0, i - half_w)
            right = min(len(session), i + half_w + 1)
            neigh_idx = [session[j] - 1 for j in range(left, right) if j != i]
            context_score = sim_mat[cur_index, neigh_idx].mean()

            rep_score = w
            hybrid_score = alpha * context_score + (1 - alpha) * rep_score

            item_scores.append((cur_id, hybrid_score, i))

        results.append((session_idx, item_scores))

    return results

def binary_search_fdataset_hybrid_wo_rm(
    original_dataset, 
    target_ratio,        # üéØ Î™©Ìëú ÎπÑÏú®
    sim_mat,
    attn_log,
    alpha=0.5,           # context vs rep_weight ÎπÑÏú®
    k=1
):
    """
    Hybrid Î∞©Ïãù (context + rep_weight) Ïù¥ÏßÑÌÉêÏÉâ ÌïÑÌÑ∞ÎßÅ.
    """
    augmented_dataset = original_dataset + prefix_cropping_sid(original_dataset)
    full_size = len(augmented_dataset)
    target_count = full_size - (full_size *target_ratio)

    score_set = meta_hybrid_scores(
        original_dataset,
        sim_mat=sim_mat,
        attn_log=attn_log,
        alpha=alpha,
    )

    score_set = make_extracted_sorted(score_set, k=k)

    fdataset_removed, removed_items = remove_items_from_dataset(original_dataset,score_set=score_set,target_count=target_count)

    faugmented_dataset = original_dataset + prefix_cropping_sid(fdataset_removed)

    return faugmented_dataset, removed_items

def make_extracted_sorted(fdataset, k=1):
    """
    fdataset: [(session_idx, [(item_id, hybrid_score, pos), ...]), ...]
    k: Í∞Å ÏÑ∏ÏÖòÏóêÏÑú ÏµúÏÜå hybrid_score Í∏∞Ï§ÄÏúºÎ°ú ÎΩëÏùÑ ÏïÑÏù¥ÌÖú Í∞úÏàò
    descending: TrueÎ©¥ hybrid_score Í∏∞Ï§Ä ÎÇ¥Î¶ºÏ∞®Ïàú Ï†ïÎ†¨, FalseÎ©¥ Ïò§Î¶ÑÏ∞®Ïàú
    """
    extracted = []

    for session_idx, session_result in fdataset:
        sorted_items = sorted(session_result, key=lambda x: x[1])
        for _, score, pos in sorted_items[:k]:
            extracted.append((session_idx, score, pos))

    # hybrid_score Í∏∞Ï§Ä Ï†ïÎ†¨
    extracted_sorted = sorted(
        extracted, key=lambda x: x[1], reverse=False
    )

    return extracted_sorted

from collections import defaultdict
def remove_items_from_dataset(original_dataset, score_set, target_count):
    count = 0

    score_dict = defaultdict(list)
    for session_idx, score, pos in score_set:
        score_dict[session_idx].append([score, pos])
    # pos Í∏∞Ï§Ä Ï†ïÎ†¨ (ÌòπÏùÄ hybrid_score Í∏∞Ï§Ä Ï†ïÎ†¨)
    for sid in score_dict:
        score_dict[sid].sort(key=lambda x: x[1])  # pos ÏàúÏÑúÎ°ú Ï†ïÎ†¨


    # ÏÑ∏ÏÖò Ïù∏Îç±Ïä§Î≥ÑÎ°ú Îπ†Î•∏ Ï†ëÍ∑ºÏö© dict
    session_map = {sid: list(session) for session, sid, _ in original_dataset}
    removed_items = []
    count = 0
    for session_idx, score, pos in score_set:
        if count >= target_count:
            break

        session = session_map[session_idx]
        if len(session) <= 2:
            continue

        # Ï†úÍ±∞
        removed_item = session[pos]
        new_session = session[:pos] + session[pos+1:]
        if len(new_session) >= 2:
            session_map[session_idx] = new_session
            removed_items.append((session_idx, pos, removed_item, score))
            count += 1

            # üîë pos ÏóÖÎç∞Ïù¥Ìä∏: Ìï¥Îãπ ÏÑ∏ÏÖò dictÏóêÏÑúÎßå Ï≤òÎ¶¨
            for entry in score_dict[session_idx]:
                if entry[1] > pos:
                    entry[1] -= 1  # pos ÏïûÏúºÎ°ú Îï°Í∏∞Í∏∞

    # ÏµúÏ¢Ö dataset Ïû¨Íµ¨ÏÑ±
    fdataset_removed = [
        (session_map[sid], sid, True)
        for _, sid, _ in original_dataset
        if sid in session_map
    ]

    return fdataset_removed, removed_items

def random_remove_items_from_dataset(original_dataset, target_count):
    # ÏÑ∏ÏÖò Ïù∏Îç±Ïä§ Î¶¨Ïä§Ìä∏
    session_indices = list(range(len(original_dataset)))
    random.shuffle(session_indices)  # ÏÑ∏ÏÖò ÏàúÏÑú ÎûúÎç§Ìôî

    # ÏÑ∏ÏÖò Î≥µÏÇ¨Î≥∏ (ÏõêÎ≥∏ Î≥¥Ï°¥)
    session_map = {i: list(original_dataset[i]) for i in session_indices}

    removed_items = []
    count = 0

    # Í∞Å ÏÑ∏ÏÖò Ìïú Î≤àÏî©Îßå Ï†ëÍ∑º
    for sid in session_indices:
        if count >= target_count:
            break

        session = session_map[sid]
        if len(session) <= 2:
            continue  # ÎÑàÎ¨¥ ÏßßÏùÄ ÏÑ∏ÏÖòÏùÄ Ìå®Ïä§

        # Î¨¥ÏûëÏúÑ ÏïÑÏù¥ÌÖú Ï†úÍ±∞
        pos = random.randint(0, len(session) - 1)
        removed_item = session[pos]

        # Ï†úÍ±∞ ÌõÑ Í∏∏Ïù¥ ÌôïÏù∏
        new_session = session[:pos] + session[pos+1:]
        if len(new_session) >= 2:
            session_map[sid] = new_session
            removed_items.append((sid, pos, removed_item))
            count += 1

    # ÏµúÏ¢Ö dataset Ïû¨Íµ¨ÏÑ±
    fdataset_removed = [session_map[i] for i in sorted(session_map.keys())]

    return fdataset_removed, removed_items

def save_and_compress_epoch_logs(epoch, score_log, score_gz_path, score_dir, remove_temp=True):

    # --- ‚úÖ score_log ---
    merged = np.concatenate(score_log, axis=0).astype(np.int32)
    score_pkl = os.path.join(score_dir, f"score_log_epoch_{epoch}.pkl")
    with open(score_pkl, "wb") as f:
        pickle.dump(merged, f, protocol=pickle.HIGHEST_PROTOCOL)

    print(f"[Epoch {epoch}] score_log saved ({os.path.getsize(score_pkl)/1024/1024:.2f} MB)")

    with gzip.open(score_gz_path, "ab") as f:
        pickle.dump({f"epoch_{epoch}": merged}, f, protocol=pickle.HIGHEST_PROTOCOL)

    # --- ÏõêÎ≥∏ ÏÇ≠Ï†ú ---
    if remove_temp:
        os.remove(score_pkl)
        print(f"[Epoch {epoch}] üßπ removed temporary .pkl files")