[
    {
      "name": "--batchSize",
      "type": "int",
      "default": 100,
      "help": "input batch size"
    },
    {
      "name": "--hiddenSize",
      "type": "int",
      "default": 256,
      "help": "hidden state size"
    },
    {
      "name": "--step",
      "type": "int",
      "default": 1,
      "help": "gnn propogation steps"
    },
    {
      "name": "--lr",
      "type": "float",
      "default": 0.001,
      "help": "learning rate"
    },
    {
      "name": "--lr_dc",
      "type": "float",
      "default": 0.1,
      "help": "learning rate decay rate"
    },
    {
      "name": "--lr_dc_step",
      "type": "int",
      "default": 3,
      "help": "the number of steps after which the learning rate decay"
    },
    {
      "name": "--l2",
      "type": "float",
      "default": 1e-05,
      "help": "l2 penalty"
    },
    {
      "name": "--validation",
      "action": "store_true",
      "default": false,
      "help": "validation"
    },
    {
      "name": "--valid_portion",
      "type": "float",
      "default": 0.1,
      "help": "split the portion of training set as validation set"
    },
    {
      "name": "--norm",
      "type": "bool",
      "default": true,
      "help": "adapt NISER, l2 norm over item and session embedding"
    },
    {
      "name": "--scale",
      "type": "bool",
      "default": true,
      "help": "scaling factor sigma"
    },
    {
      "name": "--train_flag",
      "type": "str",
      "default": "True"
    },
    {
      "name": "--PATH",
      "type": "str",
      "default": "../checkpoint/gowalla.pt",
      "help": "checkpoint path"
    },
    {
      "name": "--dot",
      "action": "store_true",
      "default": true
    },
    {
      "name": "--last_k",
      "type": "int",
      "default": 7
    },
    {
      "name": "--l_p",
      "type": "int",
      "default": 4
    },
    {
      "name": "--use_attn_conv",
      "type": "str",
      "default": "True"
    },
    {
      "name": "--heads",
      "type": "int",
      "default": 8,
      "help": "number of attention heads"
    }
  ]